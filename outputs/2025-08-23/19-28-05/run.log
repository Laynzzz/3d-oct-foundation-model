[2025-08-23 19:28:05,271][__main__][INFO] - Configuration:
[2025-08-23 19:28:05,272][__main__][INFO] - project_name: oct_cls_v1
seed: 42
paths:
  labels_tsv: ai-readi/dataset/participants.tsv
  checkpoint_path: /Users/layne/Mac/Acdamic/UCInspire/checkpoints/best_checkpoint_multi_domain.pt
  s3_bucket: eye-dataset
  s3_prefix: ai-readi/dataset/retinal_oct/
s3:
  endpoint_env: S3_ENDPOINT_URL
data:
  num_workers: 4
  cache_dir: /tmp/oct_cache
  batch_size: 2
  val_batch_size: 2
  augment:
    flip: true
    intensity_jitter: true
    resize:
    - 64
    - 384
    - 384
classes:
  mapping:
    healthy: 0
    pre_diabetes_lifestyle_controlled: 1
    oral_medication_and_or_non_insulin_injectable_medication_controlled: 2
    insulin_dependent: 3
model:
  emb_dim: 768
  freeze_encoder: true
  unfreeze_at_epoch: -1
  pool_method: mean
  head:
    hidden: 0
    dropout: 0.1
train:
  epochs: 50
  lr_head: 0.001
  lr_encoder: 3.0e-05
  weight_decay: 0.0001
  optimizer: AdamW
  scheduler: cosine
  warmup_epochs: 2
  class_weights: auto
  precision: fp32
  early_stopping:
    enabled: true
    patience: 10
    min_delta: 0.001
log:
  wandb: false
  wandb_project: 3d-oct-foundation-model
  wandb_entity: laynzzz-university-at-buffalo
  ckpt_dir: ./runs/cls_lp_v1
  save_best: true
  save_last: true

[2025-08-23 19:28:05,274][__main__][INFO] - Random seed set to 42
[2025-08-23 19:28:05,274][__main__][INFO] - Using CPU device
[2025-08-23 19:28:05,274][finetuning.data.labels][INFO] - Starting labels processing pipeline
[2025-08-23 19:28:05,728][finetuning.data.labels][INFO] - Loaded 1067 participants from B2: ai-readi/dataset/participants.tsv
[2025-08-23 19:28:05,730][finetuning.data.labels][INFO] - Filtered to 1061 participants with OCT data
[2025-08-23 19:28:05,732][finetuning.data.labels][INFO] - Class distribution:
[2025-08-23 19:28:05,732][finetuning.data.labels][INFO] -   0 (healthy): 369 samples
[2025-08-23 19:28:05,732][finetuning.data.labels][INFO] -   1 (pre_diabetes_lifestyle_controlled): 242 samples
[2025-08-23 19:28:05,732][finetuning.data.labels][INFO] -   2 (oral_medication_and_or_non_insulin_injectable_medication_controlled): 321 samples
[2025-08-23 19:28:05,732][finetuning.data.labels][INFO] -   3 (insulin_dependent): 129 samples
[2025-08-23 19:28:05,733][finetuning.data.labels][INFO] - Split sizes - Train: 743, Val: 159, Test: 159
[2025-08-23 19:28:05,733][finetuning.data.labels][INFO] - Train class distribution: {0: 290, 1: 162, 2: 233, 3: 58}
[2025-08-23 19:28:05,734][finetuning.data.labels][INFO] - Val class distribution: {0: 39, 1: 40, 2: 47, 3: 33}
[2025-08-23 19:28:05,734][finetuning.data.labels][INFO] - Test class distribution: {0: 40, 1: 40, 2: 41, 3: 38}
[2025-08-23 19:28:05,734][finetuning.data.labels][INFO] - Class weights (balanced): {0: 0.6405172413793103, 1: 1.146604938271605, 2: 0.7972103004291845, 3: 3.2025862068965516}
[2025-08-23 19:28:05,734][finetuning.data.labels][INFO] - Labels processing completed
[2025-08-23 19:28:05,734][__main__][INFO] - Dataset sizes - Train: 743, Val: 159, Test: 159
[2025-08-23 19:28:05,734][finetuning.data.locator][INFO] - Building OCT key mapping from bucket contents...
[2025-08-23 19:28:06,017][finetuning.data.locator][INFO] - Found 4 device directories under 'ai-readi/dataset/retinal_oct/structural_oct/'
[2025-08-23 19:28:06,275][finetuning.data.locator][INFO] - Found 1000 participants under heidelberg_spectralis
[2025-08-23 19:29:08,864][finetuning.data.locator][INFO] - Found 1000 participants under topcon_maestro2
[2025-08-23 19:30:32,640][finetuning.data.locator][INFO] - Found 1000 participants under topcon_triton
[2025-08-23 19:31:34,440][finetuning.data.locator][INFO] - Found 1000 participants under zeiss_cirrus
[2025-08-23 19:32:34,258][finetuning.data.locator][INFO] - Found 24750 OCT files under prefix 'ai-readi/dataset/retinal_oct/structural_oct/'
[2025-08-23 19:32:34,332][finetuning.data.locator][ERROR] - Failed to save cache to oct_key_cache.json: [Errno 2] No such file or directory: ''
[2025-08-23 19:32:34,332][finetuning.data.locator][INFO] - Built key mapping for 383 participants
[2025-08-23 19:32:34,334][finetuning.data.dataset][WARNING] - Filtered dataset: 743 -> 255 samples (488 participants without OCT data)
[2025-08-23 19:32:34,334][finetuning.data.dataset][INFO] - Initialized dataset with 255 samples
[2025-08-23 19:32:34,335][finetuning.data.locator][INFO] - Building OCT key mapping from bucket contents...
[2025-08-23 19:32:34,335][finetuning.data.locator][INFO] - Found 4 device directories under 'ai-readi/dataset/retinal_oct/structural_oct/'
[2025-08-23 19:32:34,335][finetuning.data.locator][INFO] - Found 1000 participants under heidelberg_spectralis
[2025-08-23 19:32:34,371][finetuning.data.locator][INFO] - Found 1000 participants under topcon_maestro2
[2025-08-23 19:32:34,406][finetuning.data.locator][INFO] - Found 1000 participants under topcon_triton
[2025-08-23 19:32:34,442][finetuning.data.locator][INFO] - Found 1000 participants under zeiss_cirrus
[2025-08-23 19:32:34,478][finetuning.data.locator][INFO] - Found 24750 OCT files under prefix 'ai-readi/dataset/retinal_oct/structural_oct/'
[2025-08-23 19:32:34,520][finetuning.data.locator][ERROR] - Failed to save cache to oct_key_cache.json: [Errno 2] No such file or directory: ''
[2025-08-23 19:32:34,520][finetuning.data.locator][INFO] - Built key mapping for 383 participants
[2025-08-23 19:32:34,521][finetuning.data.dataset][WARNING] - Filtered dataset: 159 -> 65 samples (94 participants without OCT data)
[2025-08-23 19:32:34,521][finetuning.data.dataset][INFO] - Initialized dataset with 65 samples
[2025-08-23 19:32:34,521][__main__][INFO] - Creating model from checkpoint: best_checkpoint_multi_domain.pt
[2025-08-23 19:32:34,521][__main__][INFO] - Freeze encoder: True
[2025-08-23 19:32:34,521][finetuning.models.model][INFO] - Creating model from checkpoint: /Users/layne/Mac/Acdamic/UCInspire/checkpoints/best_checkpoint_multi_domain.pt
[2025-08-23 19:32:35,192][finetuning.models.encoder_loader][INFO] - Loaded checkpoint from /Users/layne/Mac/Acdamic/UCInspire/checkpoints/best_checkpoint_multi_domain.pt
[2025-08-23 19:32:35,192][finetuning.models.encoder_loader][INFO] - Checkpoint epoch: 0
[2025-08-23 19:32:35,192][finetuning.models.encoder_loader][INFO] - Encoder config: {'img_size': (64, 384, 384), 'patch_size': (4, 16, 16), 'in_chans': 1, 'embed_dim': 768, 'depth': 12, 'num_heads': 12, 'mlp_ratio': 4.0, 'qkv_bias': True, 'drop_rate': 0.0, 'attn_drop_rate': 0.0, 'drop_path_rate': 0.0}
[2025-08-23 19:32:36,108][finetuning.models.encoder_loader][INFO] - Loaded encoder weights from /Users/layne/Mac/Acdamic/UCInspire/checkpoints/best_checkpoint_multi_domain.pt
[2025-08-23 19:32:36,108][finetuning.models.encoder_loader][INFO] - Encoder weights frozen for linear probing
[2025-08-23 19:32:36,110][finetuning.models.encoder_loader][INFO] - Encoder moved to device: cpu
[2025-08-23 19:32:36,159][finetuning.models.classifier][INFO] - Created linear probe head: 768 -> 4
[2025-08-23 19:32:36,160][finetuning.models.model][INFO] - Created OCT classification model:
[2025-08-23 19:32:36,160][finetuning.models.model][INFO] -   Encoder frozen: True
[2025-08-23 19:32:36,160][finetuning.models.model][INFO] -   Pooling method: mean
[2025-08-23 19:32:36,160][finetuning.models.model][INFO] -   Input shape: (64, 384, 384)
[2025-08-23 19:32:36,160][finetuning.models.model][INFO] -   Output classes: 4
[2025-08-23 19:32:36,160][finetuning.models.model][INFO] - Model parameters:
[2025-08-23 19:32:36,160][finetuning.models.model][INFO] -   encoder_total: 92,921,088
[2025-08-23 19:32:36,160][finetuning.models.model][INFO] -   encoder_trainable: 0
[2025-08-23 19:32:36,160][finetuning.models.model][INFO] -   head_total: 3,076
[2025-08-23 19:32:36,160][finetuning.models.model][INFO] -   head_trainable: 3,076
[2025-08-23 19:32:36,160][finetuning.models.model][INFO] -   total: 92,924,164
[2025-08-23 19:32:36,160][finetuning.models.model][INFO] -   trainable: 3,076
[2025-08-23 19:32:36,161][finetuning.train.loop][INFO] - Linear probe mode: training head only with lr=0.001
[2025-08-23 19:32:36,161][finetuning.train.loop][WARNING] - Auto class weights not implemented, using uniform weights
[2025-08-23 19:32:36,161][finetuning.train.loop][INFO] - Trainer initialized on device: cpu
[2025-08-23 19:32:36,161][finetuning.train.loop][INFO] - Model parameters: {'encoder_total': 92921088, 'encoder_trainable': 0, 'head_total': 3076, 'head_trainable': 3076, 'total': 92924164, 'trainable': 3076}
[2025-08-23 19:32:36,161][__main__][INFO] - Starting training...
[2025-08-23 19:32:36,161][finetuning.train.loop][INFO] - Starting training for 50 epochs
[2025-08-23 19:32:36,161][finetuning.train.loop][INFO] - Unfreeze encoder at epoch: Never
[2025-08-23 19:32:51,574][__main__][ERROR] - Training failed: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/opt/anaconda3/envs/oct_finetuning/lib/python3.11/site-packages/torch/utils/data/_utils/worker.py", line 349, in _worker_loop
    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
           ^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/oct_finetuning/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py", line 52, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/oct_finetuning/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py", line 52, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
            ~~~~~~~~~~~~^^^^^
  File "/Users/layne/Mac/Acdamic/UCInspire/3d_oct_fundation_model/finetuning/data/dataset.py", line 105, in __getitem__
    volume = read_volume(self.s3fs, oct_key, cache_dir=self.cache_dir)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/layne/Mac/Acdamic/UCInspire/3d_oct_fundation_model/finetuning/data/io.py", line 237, in read_volume
    volume = normalize_volume(volume, method="vjepa2")
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/layne/Mac/Acdamic/UCInspire/3d_oct_fundation_model/finetuning/data/io.py", line 159, in normalize_volume
    volume = torch.clamp(volume, 0, torch.quantile(volume, 0.99))
                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: quantile() input tensor is too large

